{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6.8 64-bit",
      "language": "python",
      "name": "python36864bitf46a52f191a74745a6cbe875ce1a9fb3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD6RK0bRdgqV"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MscQNr4edgq6"
      },
      "source": [
        "There are many ways to vectorize texts. One of them is tf-idf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRmxPc43dgrP"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "[tf–idf, TF*IDF, TFIDF, term frequency–inverse document frequency,](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
        "\n",
        "### TF\n",
        "\n",
        "TF -- **text frequency** — the number of times a term (word) occurs in a document. To compute, divide the number of times a term occurs in the document by the number of terms in the document.\n",
        "\n",
        "$$TF_{i,j} = \\frac{n_{i,j}}{\\sum_{k}^{}n_{i,j}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE0L54DudgrR"
      },
      "source": [
        "### IDF\n",
        "\n",
        "IDF — **inverse document frequency** —  diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
        "\n",
        "$$ IDF = log \\frac{N}{n_t}$$\n",
        "\n",
        "— N -- the number of documents, n -- the number of documents where the term occurs at least once. If the term occurs only in one document, its IDF will be high (=> the term is important for the document)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTU-8TrCdgrU"
      },
      "source": [
        "\n",
        "\n",
        "$$TFIDF = TF \\cdot IDF$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IptG-pV6dgrc"
      },
      "source": [
        "### How does it work?\n",
        "\n",
        "Consider two documents (texts):\n",
        "\n",
        "* `Bonaparte prefered pineapples`\n",
        "\n",
        "* `Josephine prefered mandarines`\n",
        "\n",
        "— 5 unique words.\n",
        "\n",
        "This gives us a 5 (words) x 2 (documents) matrix:\n",
        "\n",
        "||Bonaparte|prefered|pineapples|Josephine|mandarines|\n",
        "|:--:|:--:|:--:|:--:|:--:|:--:|\n",
        "|document 1||||||\n",
        "|document 2||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9-i9Zlmdgri"
      },
      "source": [
        "Computing TF:\n",
        "\n",
        "||Bonaparte|prefered|pineapples|Josephine|mandarines|\n",
        "|:--:|:--:|:--:|:--:|:--:|:--:|\n",
        "|document 1|1/3|1/3|1/3|0|0|\n",
        "|document 2|0|1/3|0|1/3|1/3|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRerZdedgrk"
      },
      "source": [
        "Computing IDF:\n",
        "\n",
        "||Bonaparte|prefered|pineapples|Josephine|mandarines|\n",
        "|:--:|:--:|:--:|:--:|:--:|:--:|\n",
        "|document 1|$log(2/1)$|$log(2/2)$|$log(2/1)$|||\n",
        "|document 2||$log(2/2)$||$log(2/1)$|$log(2/1)$|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIu4MZMIdgrs"
      },
      "source": [
        "TFIDF:\n",
        "\n",
        "||Bonaparte|prefered|pineapples|Josephine|mandarines|\n",
        "|:--:|:--:|:--:|:--:|:--:|:--:|\n",
        "|документ 1|0.231|0|0.231|0|0|\n",
        "|документ 2|0|0|0|0.231|0.231|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVyoyl26dgru"
      },
      "source": [
        "Document 1 can therefore be characterized by a vector `[0.231, 0, 0.231, 0, 0]`, document 2 — by a vector `[0, 0, 0, 0.231, 0.231]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztxwLvb2dgrx"
      },
      "source": [
        "### What is important to remember while computing TF-IDF?\n",
        "\n",
        "**Preprocessing**\n",
        "\n",
        "Get rid of the punctuation, lower and lemmatize your text.\n",
        "\n",
        "**Stopwords**\n",
        "\n",
        "Get rid of the stopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS7AIuqedgr1"
      },
      "source": [
        "### Computing TF-IDF in python\n",
        "\n",
        "…use the sklearn package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGhjUMzhdgr2",
        "outputId": "acd25029-4d97-4725-cf49-0451a0d98198"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tx6HRGLdgsB"
      },
      "source": [
        "stops = stopwords.words(\"russian\")\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer=\"word\", # analyze words or symbols (char)\n",
        "    stop_words=stops # to pass the list of Russian stopwords from NLTK\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IinjvEd7dgsD"
      },
      "source": [
        "## An example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrH270AcdgsF"
      },
      "source": [
        "Загрузим данные из некоторой заполярной газеты:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU8QAqBXdgsH"
      },
      "source": [
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWIgQlZBdgsK"
      },
      "source": [
        "path_to_articles = \"/content/sample_data/polar_circle\"\n",
        "articles = [os.path.join(path_to_articles, item)\n",
        "            for item in os.listdir(path_to_articles)\n",
        "            if item.endswith(\".txt\")]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35wL4F6rdgsM"
      },
      "source": [
        "articles_texts = []\n",
        "\n",
        "for article_path in articles:\n",
        "    with open(article_path, \"r\", encoding=\"utf-8\") as a_src:\n",
        "        # the first 6 lines contain metadata, the last line contains the number of comments\n",
        "        articles_texts.append(\"\\n\".join(a_src.readlines()[6:-1]))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiiO4AEYdgsQ"
      },
      "source": [
        "Lemmatizing:\n",
        "\n",
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lt2tpvgdgsR"
      },
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from pymorphy2 import MorphAnalyzer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BYXPB5MdgsS"
      },
      "source": [
        "morph = MorphAnalyzer()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKnHJ8todgsV"
      },
      "source": [
        "articles_preprocessed = []\n",
        "for a_text in articles_texts:\n",
        "    a_tokens = wordpunct_tokenize(a_text)\n",
        "    a_lemmatized = \" \".join([morph.parse(item)[0].normal_form for item in a_tokens])\n",
        "    articles_preprocessed.append(a_lemmatized)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTt_Uj81dgsZ"
      },
      "source": [
        "Computing TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmh7p2Zodgsc",
        "outputId": "e483c517-81a2-4eff-f226-5bec59ee5369"
      },
      "source": [
        "articles_tfidf = tfidf.fit_transform(articles_preprocessed)\n",
        "print(f\"Matrix of {articles_tfidf.shape[0]} documents and {articles_tfidf.shape[1]} terms\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix of 2 documents and 198 terms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AngxOe1fdgsw",
        "outputId": "328d0c3c-d695-4b00-e533-584d6d878be4"
      },
      "source": [
        "# to look at the terms in the matrix\n",
        "names = tfidf.get_feature_names_out()\n",
        "names[10:20]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['аварийный', 'август', 'автономный', 'администрация', 'апрель',\n",
              "       'введение', 'ведомство', 'весь', 'включая', 'возможность'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtiE19L7dgs2"
      },
      "source": [
        "**keyword extraction**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3htxBLwdgs3"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkuJBvuZdgs_"
      },
      "source": [
        "def get_top_tf_idf_words(tfidf_vector, feature_names, top_n):\n",
        "    sorted_nzs = np.argsort(tfidf_vector.data)[:-(top_n+1):-1]\n",
        "    return feature_names[tfidf_vector.indices[sorted_nzs]]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQrRdhrbdgtB",
        "outputId": "b6bafa19-83a8-4226-d5ef-3cb54332ddbf"
      },
      "source": [
        "feature_names = np.array(tfidf.get_feature_names_out())\n",
        "\n",
        "for i, article in enumerate(articles_texts):\n",
        "    # to print the first article\n",
        "    if i < 1:\n",
        "        article_vector = articles_tfidf[i, :]\n",
        "        words = get_top_tf_idf_words(article_vector, feature_names, 10)\n",
        "        print(article)\n",
        "        print(words)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Опыт Ямала представлен на конференции «Связь на Русском Севере»\n",
            "\n",
            "В Сочи завершилась IV конференция «Связь на Русском Севере». Ямал представлял директор департамента информационных технологий и связи региона Олег Ефремов. Он выступил с докладом по устранению «цифрового неравенства» на территории округа. Глава ведомства сообщил, что между правительством автономного округа, Минкомсвязи и ПАО «Ростелеком» заключено трёхстороннее соглашение. Одна из его главных целей – обеспечение равных возможностей для всех жителей региона в использовании современных услуг связи, включая высокоскоростной доступ к сети Интернет. В рамках соглашения предусмотрено строительство точек доступа универсальных услуг связи в населённых пунктах с численностью жителей от 250 до 500 человек. Запланирована модернизация существующих сетей связи в населённых пунктах с численностью населения от 500 до 10 тыс. жителей. Обсуждается проект введения оптоволоконного интернета в деревянные дома. Трёхстороннее соглашение охватывает восемь населённых пунктов региона с общей численностью населения 3195 человек.\n",
            "\n",
            "Также Олег Ефремов сообщил, что в 2016 году оказана поддержка региональному оператору связи ОАО «Ямалтелеком» на модернизацию транспортной сети связи оператора. На компенсацию эксплуатационных затрат операторов связи, действующих в отдалённых и труднодоступных населённых пунктах выделено 13,75 млн рублей. С апреля по август 2016 года пользователями универсальной услуги связи доступа к сети Интернет в автономном округе потреблено 414,9 Гб данных.\n",
            "\n",
            "['связь' 'сеть' 'пункт' 'населить' 'интернет' 'соглашение' 'регион'\n",
            " 'услуга' 'доступ' 'округ']\n"
          ]
        }
      ]
    }
  ]
}