{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fi-d6xoVHNFs"
   },
   "source": [
    "### Distributive hypothesis in semantics\n",
    "\n",
    "+ Ludwig Wittgenstein:\n",
    "Die Bedeutung eines Wortes liegt in seinem Gebrauch.\n",
    "\n",
    "\n",
    "+ Firth (1935:37) on context dependence (cited by Stubbs):\n",
    "the complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously.\n",
    "\n",
    "\n",
    "+ Firth (1957:11):\n",
    "You shall know a word by the company it keeps . . .\n",
    "\n",
    "\n",
    "+ Harris (1954:34):\n",
    "All elements in a language can be grouped into classes whose relative occurrence can be stated exactly. However, for the occurrence of a particular member of one class relative to a particular member of another class, it would be necessary to speak in terms of probability, based on the frequency of that occurrence in a sample.\n",
    "\n",
    "\n",
    "+ Harris (1954:34):\n",
    "It is possible to state the occurrence of any element relative to any other element, to the degree of exactness indicated above, so that distributional statements can cover all of the material of a language without requiring support from other types of information.\n",
    "\n",
    "\n",
    "+ Harris (1954:34) (anticipating deep learning?):\n",
    "The restrictions on relative occurrence of each element are described most simply by a network of interrelated statements, certain of them being put in terms of the results of certain others, rather than by a simple measure of the total restriction on each element separately.\n",
    "\n",
    "\n",
    "+ Harris (1954:36) on levels of analysis:\n",
    "\n",
    "\n",
    "    - Some question has been raised as to the reality of this structure. Does it really exist, or is it just a mathematical creation of the investigator’s? Skirting the philosophical difficulties of this problem, we should, in any case, realize that there are two quite different questions here.\n",
    "    \n",
    "    - One: Does the structure really exist in language? The answer is yes, as much as any scientific structure really obtains in the data which it describes — the scientific structure states a network of relations, and these relations really hold in the data investigated.\n",
    "    \n",
    "    - Two: Does the structure really exist in speakers? Here we are faced with a question of fact which is not directly or fully investigated in the process of determining the distributional structure. Clearly, certain behaviors of the speakers indicate perception along the lines of the distributional structure, for example, the fact that while people imitate nonlinguistic or foreign-language sounds, they repeat utterances of their own language.\n",
    "\n",
    "\n",
    "+ Harris (1954:39) on meaning and context-dependence:\n",
    "All this is not to say that there is not a great interconnection between language and meaning, in whatever sense it may be possible to use this work. But it is not a one-to-one relation between morphological structure and anything else. There is not even a one-to-one relation between vocabulary and any independent classification of meaning; we cannot say that each morpheme or word has a single central meaning or even that it has a continuous or coherent range of meanings...The correlation between language and meaning is much greater when we consider connected discourse.\n",
    "\n",
    "\n",
    "+ Harris (1954:43):\n",
    "The fact that, for example, not every adjective occurs with every noun can be used as a measure of meaning difference. For it is not merely that different members of the one class have different selections of members of the other class with which they are actually found. More than that: if we consider words or morphemes A and B to be more different than A and C, then we will often find that the distributions of A and B are more different than the distributions of A and C. In other words, difference in meaning correlates with difference in distribution.\n",
    "\n",
    "\n",
    "+ Turney & Pantel (2010:153):\n",
    "\n",
    "\n",
    "    - Statistical semantics hypothesis: Statistical patterns of human word usage can be used to figure out what people mean (Weaver, 1955; Furnas et al., 1983). – If units of text have similar vectors in a text frequency matrix, then they tend to have similar meanings. (We take this to be a general hypothesis that subsumes the four more specific hypotheses that follow.)\n",
    "\n",
    "    - Bag of words hypothesis: The frequencies of words in a document tend to indicate the relevance of the document to a query (Salton et al., 1975). – If documents and pseudo-documents (queries) have similar column vectors in a term–document matrix, then they tend to have similar meanings.\n",
    "\n",
    "    - Distributional hypothesis: Words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990). – If words have similar row vectors in a word–context matrix, then they tend to have similar meanings.\n",
    "      \n",
    "    - Extended distributional hypothesis: Patterns that co-occur with similar pairs tend to have similar meanings (Lin & Pantel, 2001). – If patterns have similar column vectors in a pair–pattern matrix, then they tend to express similar semantic relations.\n",
    "\n",
    "    - Latent relation hypothesis: Pairs of words that co-occur in similar patterns tend to have similar semantic relations (Turney et al., 2003). – If word pairs have similar row vectors in a pair–pattern matrix, then they tend to have similar semantic relations.\n",
    "    \n",
    "    \n",
    "+ What is the meaning of the word \"bardiwac\" (Stefan Evert's example)?\n",
    "\n",
    "    - He handed her her glass of bardiwac.\n",
    "\n",
    "    - Beef dishes are made to complement the bardiwacs.\n",
    "\n",
    "    - Nigel staggered to his feet, face flushed from too much bardiwac.\n",
    "\n",
    "    - Malbec, one of the lesser-known bardiwac grapes, responds well to Australia’s sunshine.\n",
    "\n",
    "    - I dined off bread and cheese and this excellent bardiwac.\n",
    "\n",
    "    - The drinks were delicious: blood-red bardiwac as well as light, sweet Rhenish.\n",
    "\n",
    "#### Word2Vec\n",
    "\n",
    "One of the most famous distributional models is word2vec. The model is based on a neural network that predicts the probability of occurrence of a given word in a given context. The two seminal papers are linked below:\n",
    "\n",
    "+ [Efficient Estimation of Word Representations inVector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "+ [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "\n",
    "The model produces word representations in a form of a vector, or, an embedding.\n",
    "\n",
    "Word2Vec comprises two algorithms: Skip-Gram and Continuous Bag-Of-Words (CBOW). The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.\n",
    "\n",
    "#### How does word2vec work?\n",
    "\n",
    "Word2Vec takes a corpus as an input and creates a vector for each word. Vectors (embeddings) are created based on the distributional hypothesis. Cosine similarity between embeddings reflects similarity in the semantics of the words.\n",
    "\n",
    "We can use embeddings to create analogies:\n",
    "\n",
    "+ king: man = queen: woman $\\Rightarrow$\n",
    "+ king - man + woman = queen\n",
    "\n",
    "![w2v](https://cdn-images-1.medium.com/max/2600/1*sXNXYfAqfLUeiDXPCo130w.png)\n",
    "\n",
    "More on the mechanics you can find [here](https://habr.com/ru/post/446530/)\n",
    "\n",
    "#### Why do we need it?\n",
    "\n",
    "+ to solve semantic problems\n",
    "+ for which classes of words is the distributional hypothesis most useful?\n",
    "+ some papers on its use in semantics:\n",
    "\n",
    "* [Turney and Pantel 2010](https://jair.org/index.php/jair/article/view/10640)\n",
    "* [Lenci 2018](https://www.annualreviews.org/doi/abs/10.1146/annurev-linguistics-030514-125254?journalCode=linguistics)\n",
    "* [Smith 2019](https://arxiv.org/pdf/1902.06006.pdf)\n",
    "* [Pennington et al. 2014](https://www.aclweb.org/anthology/D14-1162/)\n",
    "* [Faruqui et al. 2015](https://www.aclweb.org/anthology/N15-1184/)\n",
    "\n",
    "+ to create input for neural networks\n",
    "+ word2vec is used in Siri, Google Assistant, Alexa, Google Translate...\n",
    "\n",
    "#### Gensim\n",
    "\n",
    "We will use the `gensim` library to get access to the word2vec model. Here you can find the library's [documentation](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "First, let's install the library: `pip install gensim`. You can do it from jupyter: `!pip install gensim`. To update: `pip install gensim --upgrade` or `pip install gensim -U`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import logging\n",
    "import nltk.data\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCVWJx4vHNGl"
   },
   "source": [
    "#### How to train your own model\n",
    "\n",
    "NB! The training does not involve preprocessing! It means that, if necessary for your task, you have to get rid of the punctuation, lower, lemmatize, do the pos tagging before the training.\n",
    "\n",
    "To log the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dmidzjLHNGo"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qG-y6_ciHNGw"
   },
   "source": [
    "The input for the model is a text file, where every sentence starts on a new line. The text is stripped of the punctuation, lowered and lemmatized. We won't do the preprocessing part in class, we will use a preprocessed file liza_lem.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwkNLeTyHNGz"
   },
   "outputs": [],
   "source": [
    "f = 'liza_lem.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EkuDSJjHNG2"
   },
   "source": [
    "We will be training our model now. The main parameters:\n",
    "\n",
    "+ the data should be iterable\n",
    "+ vector_size — dimensionality of the word vectors,\n",
    "+ window — maximum distance between the current and predicted word within a sentence,\n",
    "+ min_count — ignores all words with total frequency lower than this,\n",
    "+ sg —  training algorithm: 1 for skip-gram; otherwise CBOW,\n",
    "+ sample — the threshold for configuring which higher-frequency words are randomly downsampled,\n",
    "+ iter — number of iterations (epochs) over the corpus,\n",
    "+ max_vocab_size — limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGKn-5wdHNG-",
    "outputId": "e133811f-7f8d-4a88-bf8d-bcaef9517004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81.3 ms, sys: 5.51 ms, total: 86.8 ms\n",
      "Wall time: 310 ms\n"
     ]
    }
   ],
   "source": [
    "%time model_liza = gensim.models.Word2Vec(data, vector_size=300, window=5, min_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ8gG5e9HNG_"
   },
   "source": [
    "We can normalize the vectors, then the model would take up less RAM. After this operation, however, you won't be able to retrain the model. L2-normalization is used: the sum of squares of all the vector elements will be brought to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYprhwMAHNHF",
    "outputId": "a66209f0-588f-4061-d57a-50fc840d1e18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model_liza.init_sims(replace=True)\n",
    "model_path = \"liza.bin\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_liza.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1DAUZGxHNHU"
   },
   "source": [
    "Let's see what the model learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0x8fs7sHNHW",
    "outputId": "8aadf9e0-7d60-4fd5-afdc-ebf10be3ec67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('проходить', 0.18653517961502075),\n",
       " ('нежный', 0.17550215125083923),\n",
       " ('показываться', 0.16081567108631134)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_liza.wv.most_similar(positive=[\"смерть\", \"любовь\"], negative=[\"печальный\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hhbn1Oa1HNHZ",
    "outputId": "4450ee47-acdd-4f07-b060-df44c7e3f92f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('выть', 0.2030768096446991),\n",
       " ('нежный', 0.1860518455505371),\n",
       " ('лодка', 0.1758255660533905)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_liza.wv.most_similar(\"любовь\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEd9n-70HNHg",
    "outputId": "64b44f3a-b3ce-488d-93eb-45d270972c35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14449573"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_liza.wv.similarity(\"лиза\", \"эраст\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hp-fhh_fHNHj",
    "outputId": "757ef8dc-f65a-4f35-80bb-4896fef821e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_liza.wv.similarity(\"лиза\", \"лиза\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "gmr5iAcoHNHm",
    "outputId": "4500c169-4d9a-44de-d427-6fbf543bf882"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'слеза'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_liza.wv.doesnt_match(\"скорбь грусть слеза улыбка\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yGJRjeUHNHo",
    "outputId": "0f8ef9db-abf7-4b5c-c307-1e2cee6c6b39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['свой',\n",
       " 'который',\n",
       " 'мочь',\n",
       " 'сей',\n",
       " 'мой',\n",
       " 'ты',\n",
       " 'часто',\n",
       " 'слеза',\n",
       " 'жить',\n",
       " 'цветок',\n",
       " 'смотреть',\n",
       " 'прощаться',\n",
       " 'прекрасный',\n",
       " 'девушка',\n",
       " 'час',\n",
       " 'дуб',\n",
       " 'поле',\n",
       " 'дело',\n",
       " 'поцеловать',\n",
       " 'деревня',\n",
       " 'довольно',\n",
       " 'страшно',\n",
       " 'какой',\n",
       " 'карман',\n",
       " 'побледнеть',\n",
       " 'забава',\n",
       " 'схватывать']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_liza.wv.words_closer_than(\"лиза\", \"эраст\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-hphDcEHNHq"
   },
   "source": [
    "#### Parameter variation\n",
    "\n",
    "Note that what is said below works for large corpora, if your corpus is small, you need to be extra careful!\n",
    "\n",
    "1) preprocessing -- do we lemmatize, tokenize, pos-tag or not\n",
    "\n",
    "2) corpus size -- the greater, the better; but! for semantic tasks the quality is more important than quantity\n",
    "\n",
    "3) vocabulary size\n",
    "\n",
    "4) negative samples\n",
    "\n",
    "5) the number of iterations\n",
    "\n",
    "6) vector size -- 100-300 (it looks like >300 does not make the results better)\n",
    "\n",
    "7) window size -- for syntax -- around 4, for semantics -- 8, 10.\n",
    "\n",
    "A paper that discusses different parameter settings: https://www.aclweb.org/anthology/D14-1162.pdf\n",
    "\n",
    "### How to use a pre-trained model\n",
    "\n",
    "http://vectors.nlpl.eu/ provides a number of pre-trained models for Russian and for other languages.\n",
    "\n",
    "For other languages, look also at [fastText](https://fasttext.cc/docs/en/english-vectors.html) and [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "For a bit of exploration, let's look at some vector novels https://nevmenandr.github.io/novel2vec/\n",
    "\n",
    "#### Working with a model\n",
    "\n",
    "Word2vec models can have two formats:\n",
    "\n",
    "+ .vec.gz — an ordinary file\n",
    "+ .bin.gz — a binary file\n",
    "\n",
    "To load a word2vec model, use `KeyedVectors`, you can set the `binary` parameter of the function `load_word2vec_format`.\n",
    "\n",
    "Note that if the embeddings were created not by word2vec, you need to use `load`. Use it if you load the `glove`, `fasttext`, `bpe` embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................................] 638171816 / 638171816"
     ]
    }
   ],
   "source": [
    "model_url = 'http://vectors.nlpl.eu/repository/20/220.zip'\n",
    "m = wget.download(model_url)\n",
    "model_file = model_url.split('/')[-1]\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ihw9StUmHNH3"
   },
   "outputs": [],
   "source": [
    "words = ['хороший_ADJ', 'плохой_ADJ', 'ужасный_ADJ','жуткий_ADJ', 'страшный_ADJ', 'красный_ADJ', 'синий_ADJ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ltyT3lGHNH4"
   },
   "source": [
    "We need the POS tags, because the model was trained on lemmatized and tagged words. The name of the model specifies the algorythm that was used to tag the words, mystem, in our case.\n",
    "\n",
    "Let's look at the 10 closest members for each word that we are interested in and at the cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "HXU-NQA5HNH6",
    "outputId": "73154b39-d9a1-42bb-f59d-9abb6564741b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хороший_ADJ\n",
      "[-0.5533533   3.525192    1.3954544   0.50957227  0.9530872   0.42150345\n",
      "  0.14798506 -0.89938575 -1.9526412  -2.9605858 ]\n",
      "плохой_ADJ 0.7704135179519653\n",
      "отличный_ADJ 0.745093822479248\n",
      "хороший_ADV 0.7096987962722778\n",
      "неплохой_ADJ 0.7080509662628174\n",
      "хорошо_ADJ 0.685799777507782\n",
      "превосходный_ADJ 0.66509610414505\n",
      "приятный_ADJ 0.6305955052375793\n",
      "хорошо_ADV 0.6236262321472168\n",
      "дурной_ADJ 0.5771215558052063\n",
      "нужный_ADJ 0.5764609575271606\n",
      "\n",
      "\n",
      "плохой_ADJ\n",
      "[ 1.47815     3.4967737   0.6200617   0.21216297 -2.1780734  -0.71112794\n",
      " -0.55119324 -0.8843036  -1.0574621  -2.4701962 ]\n",
      "хороший_ADJ 0.7704134583473206\n",
      "плохой_ADV 0.6875114440917969\n",
      "дурной_ADJ 0.6704886555671692\n",
      "плохо_ADJ 0.6194170117378235\n",
      "скверный_ADJ 0.6098257303237915\n",
      "слабый_ADJ 0.5972127914428711\n",
      "хороший_ADV 0.5740218162536621\n",
      "плохо_ADV 0.5671497583389282\n",
      "неплохой_ADJ 0.5508592128753662\n",
      "неудовлетворительный_ADJ 0.5305141806602478\n",
      "\n",
      "\n",
      "ужасный_ADJ\n",
      "[-0.7115563   0.14254573 -0.9630084  -0.7578321  -0.35663527 -2.2121139\n",
      " -1.0280207   1.6492867   0.29676497 -1.4163486 ]\n",
      "страшный_ADJ 0.8240757584571838\n",
      "жуткий_ADJ 0.7339372038841248\n",
      "ужасать_VERB 0.7301003932952881\n",
      "отвратительный_ADJ 0.7261567115783691\n",
      "чудовищный_ADJ 0.7105397582054138\n",
      "жестокий_ADJ 0.6685523986816406\n",
      "кошмарный_ADJ 0.6566017866134644\n",
      "печальный_ADJ 0.6309635639190674\n",
      "ужас_NOUN 0.6154152750968933\n",
      "невыносимый_ADJ 0.6138052940368652\n",
      "\n",
      "\n",
      "жуткий_ADJ\n",
      "[-2.6076715  -0.55262923  0.40085354  0.53245926 -0.627791   -1.1019641\n",
      " -0.9849679   1.96757     0.7459387  -1.8215437 ]\n",
      "страшный_ADJ 0.7592697143554688\n",
      "ужасный_ADJ 0.7339370846748352\n",
      "зловещий_ADJ 0.7206223607063293\n",
      "жутковатый_ADJ 0.7082728147506714\n",
      "мрачный_ADJ 0.7029562592506409\n",
      "кошмарный_ADJ 0.670348048210144\n",
      "жуть_NOUN 0.6547104716300964\n",
      "странный_ADJ 0.6514071226119995\n",
      "отвратительный_ADJ 0.6419169306755066\n",
      "ужасать_VERB 0.6389007568359375\n",
      "\n",
      "\n",
      "страшный_ADJ\n",
      "[-2.2825887  -1.2624556   1.1256089  -0.6564649  -0.09272972 -2.5593076\n",
      " -1.475779    2.8121662   0.97330487 -1.0684136 ]\n",
      "ужасный_ADJ 0.8240758180618286\n",
      "жуткий_ADJ 0.759269654750824\n",
      "чудовищный_ADJ 0.6618976593017578\n",
      "ужасать_VERB 0.6316395998001099\n",
      "ужас_NOUN 0.6198166012763977\n",
      "жестокий_ADJ 0.614973783493042\n",
      "зловещий_ADJ 0.6030802726745605\n",
      "кровавый_ADJ 0.5815890431404114\n",
      "мрачный_ADJ 0.5714954137802124\n",
      "страшилище_NOUN 0.5660818815231323\n",
      "\n",
      "\n",
      "красный_ADJ\n",
      "[ 1.8484586  -2.8723414  -0.46619502 -2.2685125  -2.5031633  -1.0955416\n",
      " -0.6339649   2.7543478   0.505872    0.05006269]\n",
      "*красный_ADJ 0.5675032734870911\n",
      "крайовый_ADJ 0.5446105003356934\n",
      "малиновый_ADJ 0.5436140298843384\n",
      "алый_ADJ 0.5398156046867371\n",
      "белый_ADJ 0.5338898301124573\n",
      "красный_NOUN 0.5177537798881531\n",
      "людова_PROPN 0.5174776315689087\n",
      "красный_PROPN 0.513830304145813\n",
      "малиновой_ADJ 0.49443739652633667\n",
      "черный_ADJ 0.4935334026813507\n",
      "\n",
      "\n",
      "синий_ADJ\n",
      "[ 2.3165712  -2.4371688  -1.9381734   0.4337334  -0.40584928 -0.42849195\n",
      " -0.5164158   2.7145562  -0.58174306 -0.03131035]\n",
      "голубой_ADJ 0.879791796207428\n",
      "желтый_ADJ 0.82841557264328\n",
      "синий_NOUN 0.825542151927948\n",
      "зеленый_ADJ 0.7674750089645386\n",
      "фиолетовый_ADJ 0.7418162822723389\n",
      "оранжевый_ADJ 0.7393277287483215\n",
      "серый_ADJ 0.734974205493927\n",
      "розовый_ADJ 0.7345995306968689\n",
      "жёлтый_ADJ 0.7321051955223083\n",
      "лиловый_ADJ 0.7305857539176941\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # is the word present in the model?\n",
    "    if word in model:\n",
    "        print(word)\n",
    "        # looking at the first 10 numbers from the embedding\n",
    "        print(model[word][:10])\n",
    "        # getting 10 neighbours\n",
    "        for i in model.most_similar(positive=[word], topn=10):\n",
    "            # word + cosine similarity\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Oops!\n",
    "        print('Oops, the word \"%s\" is not in the model!' % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQleI2RSHNH-"
   },
   "source": [
    "Cosine similarity for a pair of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7yCoODJHNH_",
    "outputId": "9d52e1bd-02fb-4049-eb76-337177cb88e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7704135\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('плохой_ADJ', 'хороший_ADJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nj_YXxnHNIA",
    "outputId": "c2628c9c-d358-477f-d76b-d45907cfc66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027845463\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('плохой_ADJ', 'синий_ADJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLh-SjvCHNIC",
    "outputId": "0ad399b0-e3e2-4aee-f834-08a1cafca465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73393726\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('ужасный_ADJ', 'жуткий_ADJ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URFKk_BAHNID"
   },
   "source": [
    "Proportion:\n",
    "\n",
    "+ positive — vectors that we add\n",
    "+ negative — vectors that we subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tAzDqJ6HNIF",
    "outputId": "015a456a-cc57-4197-e6c2-ee7c83e640ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "страшный_ADJ\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['плохой_ADJ', 'ужасный_ADJ'], negative=['хороший_ADJ'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XApnxdg-HNIH"
   },
   "source": [
    "Find the word that does not match the rest of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzguX9wNHNIJ",
    "outputId": "023e6fbb-bbb6-45ad-a9bf-447e37d25d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хороший_ADJ\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('плохой_ADJ хороший_ADJ ужасный_ADJ страшный_ADJ'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhEBxE72RiHq",
    "outputId": "2ec070db-57cc-4e78-ce5f-35d2ec3452dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "плохой_ADJ\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('плохой_ADJ ужасный_ADJ страшный_ADJ'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2T15LhbeHNIK",
    "outputId": "eec835cd-de78-493e-a97f-db8517941f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5586\tстрашно_ADV\n",
      "0.4865\tбезумно_ADV\n",
      "0.4491\tнесказанно_ADV\n",
      "0.433\tбезмерно_ADV\n",
      "0.4316\tнеимоверно_ADV\n",
      "0.4027\tдонельзя_ADV\n",
      "0.3973\tнеобыкновенно_ADV\n",
      "0.3899\tчрезвычайный_ADV\n",
      "0.3874\tчрезвычайность_NOUN\n",
      "0.3825\tжутко_ADV\n"
     ]
    }
   ],
   "source": [
    "for word, score in model.most_similar(positive=['ужасно_ADV'], negative=['плохой_ADJ']):\n",
    "    print(f'{score:.4}\\t{word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxCO--ZnHNIh"
   },
   "source": [
    "#### Model evaluation\n",
    "\n",
    "+ word similarity, compare the results of the training with experimental results from human participants\n",
    "+ analogies:\n",
    "\n",
    "| слово 1    | слово 2    | отношение     |\n",
    "|------------|------------|---------------|\n",
    "| Россия     | Москва     | страна-столица|\n",
    "| Норвегия   | Осло       | страна-столица|"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
