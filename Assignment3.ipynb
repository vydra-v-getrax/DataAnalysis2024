{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067b5f02",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Please send an .ipynb file, a .py file or a link to a public github repository with your file to alxdra.konovalova@gmail.com\n",
    "\n",
    "Due December 11th, 23:59, for max. 10 points\n",
    "\n",
    "8 points max. for late submission, December 18th, by 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c1ef1",
   "metadata": {},
   "source": [
    "#### Task 1: Initial Consonant Clusters \\[3 points\\]\n",
    "\n",
    "Your task is to show me how to use pattern matching to investigate the distribution of word-initial consonant clusters in *Alice* ([alice.txt](https://github.com/vydra-v-getrax/DataAnalysis2024/blob/main/week2/alice.txt)). The basic question is how frequent different kinds of initial consonant clusters are. \n",
    "\n",
    "An initial consonant cluster is a sequence of consonants (not interrupted by vowels) at the beginning of a word. E.g., *crouton* has an initial consonant cluster *cr*, *flamingo* has an initial consonant cluster *fl*, *mimosa* does not have an initial consonant cluster, it starts with a single consonant. \n",
    "\n",
    "A challenge here is that English orthography only indirectly reflects the phonology. First, you must deal with silent letters. These include cases like *know* or *gnostic* where *k* and *g* are silent. Another issue to wrestle with is that sometimes a single consonant is written with several letters, e.g. [θ] as in *thimble* or [ʃ] as in *show*, etc. Finally, there are cases where the same letter (sequence) has multiple pronunciations. For example, *c* is pronounced [s] in *city*, but [k] in *coat*. Similarly, *ch* is pronounced [tʃ] in *church*, but [k] in *chord*. Note that sometimes the difference is predictable, as in the case of *c*, but sometimes it is not, as in the case of *ch*. You should set aside the mapping of orthography to precise phonological forms, except insofar as you need to decide what a cluster is.\n",
    "\n",
    "You will need to do the following (or something which is analogous to the following):\n",
    "\n",
    "1. tokenize alice.txt (use your function from the previous homework or use any library such as **nltk, razdel** etc);\n",
    "2. apply the `gutenberg_file_wc` function (from hw2) to alice.txt to get the word counts -- 0,5 point;\n",
    "3. try to search for (word) initial consonant clusters using regular expression(s) (Don't forget to `import re` at the beginning of your program) -- 0.5 point;\n",
    "4. print the counts for all the clusters that you have identified, you should end up with a dictionary like `{'pr': 26, 'sp': 15,...}` -- 1.5 point; \n",
    "5. print the total number of different clusters you've identified, e.g., 10 clusters or 40 clusters and print the list of all the clusters in the alphabetical order, ['bl', 'br',...] -- 0,5 point;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51a33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "074eab48",
   "metadata": {},
   "source": [
    "#### Task 2: Cleaning out Named Entities [3 points]\n",
    "\n",
    "Names, dates and facts are important, but they form noise for vector models. In this task, you have to apply tools for extracting named entities and process the text without them. \n",
    "\n",
    "1. Load file [Celebrities.txt](https://github.com/vydra-v-getrax/DataAnalysis2024/blob/main/Celebrities.txt)\n",
    "2. Use **natasha** or **spacy** to collect the following named entities by groups: 1) persons (name and surname as one unit) 2) dates. You have to get two lists:` people = [...], dates = [...]`. Try to keep their start and stop indices. -- 1 point\n",
    "3. Describe and give examples of any 2 other groups of named entities that are found in the text and extract them to separate lists. -- 0.5 point\n",
    "4. Delete all the names and dates from the original text. Print the clean text. -- 0.5 point\n",
    "5. Calculate the number of nouns, verbs and adjectives in the remaining text. Use pymorphy2/3 or mystem3. Return the result as dictionary: `{'nouns': ..., 'verbs': ..., 'adjectives': ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d468861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc110b69",
   "metadata": {},
   "source": [
    "#### Task 3: Vectorizer [4 points]\n",
    "\n",
    "The goal here is to create somewhat like a search system that would take one text and return the most similar texts from the given database.\n",
    "1. Load the dataset: you will only need the first column 'text'. Store the data as a list or an iterator (or in a pandas dataframe or any table-like data structure):\n",
    "[Posts.csv](https://github.com/vydra-v-getrax/DataAnalysis2024/blob/main/Posts.csv)\n",
    "\n",
    "\n",
    "2. Tokenize and lemmatize each text. Join lemmas into one line separated by spaces so that every text is only one line with nothing but lemmas and spaces. The structure of your corpus should be similar to that: ['word1 word2 word3', 'word4, 'word5']. -- 1 point\n",
    "\n",
    "\n",
    "3. Fit and transform Tf-IDF on your corpus. For that, import `from sklearn.feature_extraction.text import TfidfVectorizer` -- 1 point\n",
    "\n",
    "\n",
    "4. Take any 5 texts from your corpus. Take the same TfidfVectorizer that you used to fit/transform your corpus and call `transform` method on these 5 sample texts. For each text, iterate over the corpus and find the most similar document from the corpus. To calculate similarity, you might use any function that computes distance between two numeric vectors such as cosine distance/similarity. For example, `from sklearn.metrics.pairwise import cosine_similarity`, or \n",
    "`from scipy import spatial` and compute similarity as `1 - spatial.distance.cosine(list1, list2)`. Show one closest document from the corpus. Bonus: sort the corpus by similarity and show top-10 closest documents. -- 2 points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a282c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
