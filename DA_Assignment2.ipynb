{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaTgSlXN2bxx"
   },
   "source": [
    "## Assignment 2\n",
    "\n",
    "Please send an .ipynb file, a .py file or a link to a public github repository with your file to alxdra.konovalova@gmail.com\n",
    "\n",
    "Due December 4th, 21:00, for max. 10 points\n",
    "\n",
    "8 points max. for late submission, December 11th, by 21:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sn3rBBMt2UDA"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from itertools import dropwhile\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTW6PxRS2UDC"
   },
   "source": [
    "1. #### Tokenizing -- 2 point\n",
    "\n",
    "Tokenizing. This is one of the most common (and vexing) operations when processing data.  There is no single correct way to do it.  The correct method will depend on the task at hand.  This problem is designed to give you a sense for the challenges and the possibilities. Give it some thought, but don't go overboard.  We have already seen some good tokenizers in different python modules, which we will be using, but this is your attempt to create your own tokenizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WpK-o7lW2UDG"
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    \"\"\"Break str s into a list of strings according to some procedure\n",
    "    tailored for the task at hand.\"\"\"\n",
    "\n",
    "    #your code\n",
    "    return #specify the output of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeVo7FKI2UDH"
   },
   "source": [
    "2. #### Word counts -- 2 points\n",
    "\n",
    "Download this file to your computer and make sure it is in the same\n",
    "directory/folder as this homework file:\n",
    "https://github.com/vydra-v-getrax/DataAnalysis2024/blob/main/week2/alice.txt\n",
    "\n",
    "Open alice.txt in your text editor and study its composition. Clearly,\n",
    "we would need to do a lot of special processing of this file in order\n",
    "to get accurate word counts for Alice.  However, for this problem, you\n",
    "need to make only a minimal effort:\n",
    "\n",
    "Complete `gutenberg_file_wc` so that it can read in the file alice.txt\n",
    "and tokenize only the lines that occur between the two lines from the\n",
    "file that start this way:\n",
    "```\n",
    "*** START OF THIS PROJECT GUTENBERG EBOOK\n",
    "*** END OF THIS PROJECT GUTENBERG EBOOK\n",
    "```\n",
    "There are a lot of strategies for doing this.  The one that is\n",
    "NOT ALLOWED is deleting material from alice.txt and saving the\n",
    "result.  Your function has to work without any modifications to\n",
    "alice.txt.\n",
    "\n",
    "A hint: you can try doing this:\n",
    "```\n",
    "alice = open('alice.txt').read()\n",
    "a = alice.find(\"*** START OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "z = alice.find(\"*** END OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "alice_new = alice[a:z]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nH-9I1E2UDJ"
   },
   "outputs": [],
   "source": [
    "def gutenberg_file_wc(filename):\n",
    "    \"\"\"Open the file with name filename, tokenize it with your\n",
    "    tokenize function, and return a dictionary mapping words to their\n",
    "    counts in the file.  Tokenize only the material that occurs\n",
    "    between the two lines given above.\"\"\"\n",
    "    #insert your code here\n",
    "    return #the output of the function\n",
    "print(gutenberg_file_wc('alice.txt'))#testing the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAZHYPYC2UDK"
   },
   "source": [
    "3. #### Counts by year -- 2 points\n",
    "\n",
    "The raw data files for the Google Books collection are available for\n",
    "download. The files are huge, so I created a sample. Download this\n",
    "file to your computer and make sure it is in the same directory/folder\n",
    "as this homework file:\n",
    "https://github.com/vydra-v-getrax/DataAnalysis2024/blob/main/googlebooks.txt\n",
    "\n",
    "The format of this file is as follows (whitespace inserted for\n",
    "readability):\n",
    "```\n",
    "word TAB year TAB match_count TAB volume_count NEWLINE\n",
    "```\n",
    "\n",
    "The TAB character is `\\t`, which you can treat like any other (for\n",
    "example, you can split a string on `\\t`).\n",
    "\n",
    "Your task: complete `googlebooks_counts_by_year` so that it processes\n",
    "my sample file and returns a 2d dictionary with this structure:\n",
    "```\n",
    "{\n",
    "  word1: {year1: count, year2: count ...},\n",
    "  word2: {year1: count, year2: count ...},\n",
    "  ...\n",
    "}\n",
    "```\n",
    "where the nature of the year dicts is determined by the file.\n",
    "(That is, different words will have different years and counts\n",
    "associated with them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uln8TG412UDM"
   },
   "outputs": [],
   "source": [
    "def googlebooks_counts_by_year(filename):\n",
    "    \"\"\"Maps a Google books 1-grams file to a 2-dimensional dictionary\n",
    "    giving each word's counts by year.\"\"\"\n",
    "\n",
    "    #insert your code here\n",
    "    return #specify the output of your function\n",
    "print(googlebooks_counts_by_year('googlebooks.txt'))#testing the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdTgg1RX2UDO"
   },
   "source": [
    "4. #### Collapse by year -- 2 points\n",
    "\n",
    "Complete the function `googlebooks_year_collapse` so that it takes\n",
    "as input the output of `googlebooks_counts_by_year` and collapses\n",
    "it down so that each word is associated with its single tokencount\n",
    "for the full, obtained by summing up all of the counts for the\n",
    "years associated with that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S228HtEE2UDO"
   },
   "outputs": [],
   "source": [
    "def googlebooks_year_collapse(d):\n",
    "    \"\"\"Convert the output of googlebooks_counts_by_year to\n",
    "    a simpler dict mapping words to counts.\"\"\"\n",
    "    #your code should be here\n",
    "    return #the output of the function\n",
    "print(googlebooks_year_collapse(googlebooks_counts_by_year('googlebooks.txt')))#testing the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyqoUTQy4Z0M"
   },
   "source": [
    "5. #### Lemmatize a poem -- 2 points\n",
    "\n",
    "a. Take the poem: https://github.com/vydra-v-getrax/DataAnalysis2024/tree/main/week4/Lukomorje.txt\n",
    "\n",
    "b. Print a lemmatized version of the poem\n",
    "\n",
    "c. Print the poem with all the verbs in the imperative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
